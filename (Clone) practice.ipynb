{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "176749bf-a17c-4682-a8c3-4858affe9592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum,avg,col,min,max,upper, lower, trim, ltrim, rtrim, substring \n",
    "from pyspark.sql.functions import substring_index, split, repeat, rpad, lpad, regexp_replace, regexp_extract, length, instr, initcap\n",
    "spark=SparkSession.builder.appName(\"func\").getOrCreate()\n",
    "# data= [\n",
    "#     (101, \"Alice\", \"HR\", 40000, 29),\n",
    "#     (102, \"Bob\", \"IT\", 55000, 34),\n",
    "#     (103, \"Charlie\", \"Finance\", 48000, 28),\n",
    "#     (104, \"David\", \"IT\", 61000, 41),\n",
    "#     (105, \"Eve\", \"HR\", 39000, 25),\n",
    "#     (106, \"Frank\", \"Finance\", 53000, 38)\n",
    "]\n",
    "\n",
    "df=spark.createDataFrame(data,[\"id\",\"name\",\"dept\",\"sal\",\"age\"])\n",
    "\n",
    "df.show()\n",
    "df.select(\"*\").show()\n",
    "df.select(\"id\",\"dept\").show()\n",
    "df.where(col(\"dept\") ==\"IT\").show()\n",
    "df.filter(col(\"age\")>29).show()\n",
    "df.filter(col(\"sal\").between(30000, 40000)).show()\n",
    "df.groupBy(\"dept\").agg(avg(col(\"sal\").alias(\"dept_avg_sal\"))).show()\n",
    "print(df.columns)\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0102e7b5-9fd5-4f15-923d-9a7122163f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import time\n",
    "\n",
    "num_records = 5000000\n",
    "df = spark.range(1, num_records + 1) \\\n",
    "    .withColumn(\"region\", (col(\"id\") % 10)) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 5)) \\\n",
    "    .withColumn(\"sales\", (rand(seed=42) * 1000).cast(\"double\"))\n",
    "\n",
    "display(df.limit(10))\n",
    "print(f\" Total Records: {df.count()}\")\n",
    "\n",
    "\n",
    "print(\" Running query without caching...\")\n",
    "\n",
    "start = time.time()\n",
    "df.filter(col(\"region\") == 3).groupBy(\"category\").avg(\"sales\").collect()\n",
    "end = time.time()\n",
    "\n",
    "print(f\" Without cache: {end - start:.2f} seconds\")\n",
    "\n",
    "\n",
    "print(\" Caching DataFrame in memory...\")\n",
    "df.cache()\n",
    "df.count()  \n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df.filter(col(\"region\") == 3).groupBy(\"category\").avg(\"sales\").collect()\n",
    "end = time.time()\n",
    "\n",
    "print(f\" With cache: {end - start:.2f} seconds\")\n",
    "\n",
    "\n",
    "print(\" Persisting with MEMORY_AND_DISK storage level...\")\n",
    "df.unpersist()\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df.count()\n",
    "\n",
    "start = time.time()\n",
    "df.filter(col(\"region\") == 3).groupBy(\"category\").avg(\"sales\").collect()\n",
    "end = time.time()\n",
    "\n",
    "print(f\" With persist (MEMORY_AND_DISK): {end - start:.2f} seconds\")\n",
    "\n",
    "df.unpersist()\n",
    "print(\" Cache cleared. Demo complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
